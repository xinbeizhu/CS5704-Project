{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b4e1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AdamW\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e024cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e9579",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a07231f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_labeled.csv')\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Combine columns and tokenize\n",
    "def combine_and_tokenize(row):\n",
    "    # Combine the text columns with some separators\n",
    "    combined_text = f\"{row['Question Title']} [SEP] {row['Description']} [SEP] {row['Accepted Answer']}\"\n",
    "    return tokenizer(combined_text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "data['inputs'] = data.apply(combine_and_tokenize, axis=1)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract inputs and labels for training and validation sets\n",
    "train_inputs = {key: torch.tensor([val[key] for val in train_data['inputs'].values]) for key in ['input_ids', 'attention_mask']}\n",
    "train_labels = torch.tensor(train_data['Label'].values)\n",
    "val_inputs = {key: torch.tensor([val[key] for val in val_data['inputs'].values]) for key in ['input_ids', 'attention_mask']}\n",
    "val_labels = torch.tensor(val_data['Label'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912b798",
   "metadata": {},
   "source": [
    "## Create a Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a830256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackOverflowDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: self.encodings[key][idx] for key in self.encodings}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = StackOverflowDataset(train_inputs, train_labels)\n",
    "val_dataset = StackOverflowDataset(val_inputs, val_labels)\n",
    "\n",
    "# Create DataLoaders for training and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cceb37",
   "metadata": {},
   "source": [
    "## Customize CodeBERT for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "475dc27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14437fb",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee2ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuhe\\AppData\\Local\\Temp\\ipykernel_3508\\1173364515.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, Loss: 18.08500862121582\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "accumulation_steps = 4  # Accumulate gradients over 4 mini-batches\n",
    "optimizer.zero_grad()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / accumulation_steps  # Normalize our loss (if averaging)\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item() * accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35590d7",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ce02fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuhe\\AppData\\Local\\Temp\\ipykernel_3508\\1173364515.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/30\n",
      "Processing batch 2/30\n",
      "Processing batch 3/30\n",
      "Processing batch 4/30\n",
      "Processing batch 5/30\n",
      "Processing batch 6/30\n",
      "Processing batch 7/30\n",
      "Processing batch 8/30\n",
      "Processing batch 9/30\n",
      "Processing batch 10/30\n",
      "Processing batch 11/30\n",
      "Processing batch 12/30\n",
      "Processing batch 13/30\n",
      "Processing batch 14/30\n",
      "Processing batch 15/30\n",
      "Processing batch 16/30\n",
      "Processing batch 17/30\n",
      "Processing batch 18/30\n",
      "Processing batch 19/30\n",
      "Processing batch 20/30\n",
      "Processing batch 21/30\n",
      "Processing batch 22/30\n",
      "Processing batch 23/30\n",
      "Processing batch 24/30\n",
      "Processing batch 25/30\n",
      "Processing batch 26/30\n",
      "Processing batch 27/30\n",
      "Processing batch 28/30\n",
      "Processing batch 29/30\n",
      "Processing batch 30/30\n",
      "Evaluation complete. Calculating metrics...\n",
      "0.55\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for idx, batch in enumerate(val_loader):\n",
    "    print(f\"Processing batch {idx+1}/{len(val_loader)}\")  # Track batch processing\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_labels = logits.argmax(dim=1).cpu().numpy()\n",
    "        predictions.extend(predicted_labels)\n",
    "        true_labels.extend(batch['labels'].numpy())\n",
    "\n",
    "print(\"Evaluation complete. Calculating metrics...\")\n",
    "print(accuracy_score(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f41a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
